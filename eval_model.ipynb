{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ## Graph attention layer\n",
    "\n",
    "    This is a single graph attention layer.\n",
    "    A GAT is made up of multiple such layers.\n",
    "\n",
    "    It takes\n",
    "    $$\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$$,\n",
    "    where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input\n",
    "    and outputs\n",
    "    $$\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$$,\n",
    "    where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `n_heads`, $K$, is the number of attention heads\n",
    "        * `is_concat` whether the multi-head results should be concatenated or averaged\n",
    "        * `dropout` is the dropout probability\n",
    "        * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Calculate the number of dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # If we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # If we are averaging the multiple heads\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # Linear layer for initial transformation;\n",
    "        # i.e. to transform the node embeddings before self-attention\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        # Linear layer to compute attention score $e_{ij}$\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        # The activation for attention score $e_{ij}$\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        # Softmax to compute attention $\\alpha_{ij}$\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n",
    "        * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n",
    "        We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n",
    "\n",
    "        Adjacency matrix represent the edges (or connections) among nodes.\n",
    "        `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        # The initial transformation,\n",
    "        # $$\\overrightarrow{g^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$$\n",
    "        # for each head.\n",
    "        # We do single linear transformation and then split it up for each head.\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        # #### Calculate attention score\n",
    "        #\n",
    "        # We calculate these for each head $k$. *We have omitted $\\cdot^k$ for simplicity*.\n",
    "        #\n",
    "        # $$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =\n",
    "        # a(\\overrightarrow{g_i}, \\overrightarrow{g_j})$$\n",
    "        #\n",
    "        # $e_{ij}$ is the attention score (importance) from node $j$ to node $i$.\n",
    "        # We calculate this for each head.\n",
    "        #\n",
    "        # $a$ is the attention mechanism, that calculates the attention score.\n",
    "        # The paper concatenates\n",
    "        # $\\overrightarrow{g_i}$, $\\overrightarrow{g_j}$\n",
    "        # and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$\n",
    "        # followed by a $\\text{LeakyReLU}$.\n",
    "        #\n",
    "        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "        # \\mathbf{a}^\\top \\Big[\n",
    "        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n",
    "        # \\Big] \\Big)$$\n",
    "\n",
    "        # First we calculate\n",
    "        # $\\Big[\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j} \\Big]$\n",
    "        # for all pairs of $i, j$.\n",
    "        #\n",
    "        # `g_repeat` gets\n",
    "        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N},\n",
    "        # \\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N}, ...\\}$$\n",
    "        # where each node embedding is repeated `n_nodes` times.\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "        # `g_repeat_interleave` gets\n",
    "        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_1}, \\dots, \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_2}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_2}, ...\\}$$\n",
    "        # where each node embedding is repeated `n_nodes` times.\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        # Now we concatenate to get\n",
    "        # $$\\{\\overrightarrow{g_1} \\Vert \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_1} \\Vert \\overrightarrow{g_2},\n",
    "        # \\dots, \\overrightarrow{g_1}  \\Vert \\overrightarrow{g_N},\n",
    "        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_2},\n",
    "        # \\dots, \\overrightarrow{g_2}  \\Vert \\overrightarrow{g_N}, ...\\}$$\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        # Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "\n",
    "        # Calculate\n",
    "        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "        # \\mathbf{a}^\\top \\Big[\n",
    "        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n",
    "        # \\Big] \\Big)$$\n",
    "        # `e` is of shape `[n_nodes, n_nodes, n_heads, 1]`\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        # Remove the last dimension of size `1`\n",
    "        e = e.squeeze(-1)\n",
    "\n",
    "        # The adjacency matrix should have shape\n",
    "        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "        # Mask $e_{ij}$ based on adjacency matrix.\n",
    "        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
    "\n",
    "        # We then normalize attention scores (or coefficients)\n",
    "        # $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =\n",
    "        # \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
    "        #\n",
    "        # where $\\mathcal{N}_i$ is the set of nodes connected to $i$.\n",
    "        #\n",
    "        # We do this by setting unconnected $e_{ij}$ to $- \\infty$ which\n",
    "        # makes $\\exp(e_{ij}) \\sim 0$ for unconnected pairs.\n",
    "        a = self.softmax(e)\n",
    "\n",
    "        # Apply dropout regularization\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        # Calculate final output for each head\n",
    "        # $$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{g^k_j}$$\n",
    "        #\n",
    "        # *Note:* The paper includes the final activation $\\sigma$ in $\\overrightarrow{h_i}$\n",
    "        # We have omitted this from the Graph Attention Layer implementation\n",
    "        # and use it on the GAT model to match with how other PyTorch modules are defined -\n",
    "        # activation as a separate layer.\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            # $$\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        # Take the mean of the heads\n",
    "        else:\n",
    "            # $$\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Train a Graph Attention Network (GAT) on Cora dataset\n",
    "summary: >\n",
    "  This trains is a  Graph Attention Network (GAT) on Cora dataset\n",
    "---\n",
    "\n",
    "# Train a Graph Attention Network (GAT) on Cora dataset\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from download import download_file, extract_tar\n",
    "\n",
    "# from labml import lab, monit, tracker, experiment\n",
    "# from labml.configs import BaseConfigs, option, calculate\n",
    "# from labml.utils import download\n",
    "# from labml_helpers.device import DeviceConfigs\n",
    "# from labml_helpers.module import Module\n",
    "# from labml_nn.graphs.gat import GraphAttentionLayer\n",
    "# from labml_nn.optimizers.configs import OptimizerConfigs\n",
    "\n",
    "\n",
    "class CoraDataset:\n",
    "    def __init__(self, include_edges: bool = True):\n",
    "        self.include_edges = include_edges\n",
    "\n",
    "        # Download dataset\n",
    "        self._download()\n",
    "\n",
    "        # Read the paper ids, feature vectors, and labels\n",
    "        content = np.genfromtxt('cora/cora/cora.content', dtype=np.dtype(str))\n",
    "\n",
    "        # Load the citations, it's a list of pairs of integers.\n",
    "        citations = np.genfromtxt('cora/cora/cora.cites', dtype=np.int32)\n",
    "\n",
    "        features = torch.tensor(np.array(content[:, 1:-1], dtype=np.float32))\n",
    "        self.features = features / features.sum(dim=1, keepdim=True)\n",
    "\n",
    "        self.classes = {s: i for i, s in enumerate(set(content[:, -1]))}\n",
    "        self.labels = torch.tensor([self.classes[i] for i in content[:, -1]], dtype=torch.long)\n",
    "\n",
    "        paper_ids = np.array(content[:, 0], dtype=np.int32)\n",
    "        ids_to_idx = {id_: i for i, id_ in enumerate(paper_ids)}\n",
    "\n",
    "        self.adj_mat = torch.eye(len(self.labels), dtype=torch.bool)\n",
    "\n",
    "        if self.include_edges:\n",
    "            for e in citations:\n",
    "                e1, e2 = ids_to_idx[e[0]], ids_to_idx[e[1]]\n",
    "                self.adj_mat[e1][e2] = True\n",
    "                self.adj_mat[e2][e1] = True\n",
    "\n",
    "    def _download(self):\n",
    "        data_dir = Path('cora')\n",
    "        if not data_dir.exists():\n",
    "            data_dir.mkdir(parents=True)\n",
    "\n",
    "        tgz_file = data_dir / 'cora.tgz'\n",
    "        content_file = data_dir / 'cora.content'\n",
    "\n",
    "        if not content_file.exists():\n",
    "            download_file('https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz', tgz_file)\n",
    "            extract_tar(tgz_file, data_dir)\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = GraphAttentionLayer(in_features, n_hidden, n_heads, is_concat=True, dropout=dropout)\n",
    "        self.activation = nn.ELU()\n",
    "        self.output = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, adj_mat):\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer1(x, adj_mat)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x, adj_mat)\n",
    "\n",
    "\n",
    "def accuracy(output: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    A simple function to calculate the accuracy\n",
    "    \"\"\"\n",
    "    return output.argmax(dim=-1).eq(labels).sum().item() / len(labels)\n",
    "\n",
    "\n",
    "class CoraConfig:\n",
    "    def __init__(self):\n",
    "        self.include_edges = True\n",
    "        self.epochs = 10 #1000\n",
    "        self.model = None\n",
    "        self.training_samples = 5 #500\n",
    "        self.in_features = None\n",
    "        self.n_hidden = 64\n",
    "        self.n_heads = 8\n",
    "        self.n_classes = None\n",
    "        self.dropout = 0.6\n",
    "        self.dataset = CoraDataset(self.include_edges)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.optimizer = None #torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "    def run(self):\n",
    "        features = self.dataset.features.to(self.device)\n",
    "        labels = self.dataset.labels.to(self.device)\n",
    "        edges_adj = self.dataset.adj_mat.to(self.device)\n",
    "        edges_adj = edges_adj.unsqueeze(-1)\n",
    "\n",
    "        idx_rand = torch.randperm(len(labels))\n",
    "        idx_train = idx_rand[:self.training_samples]\n",
    "        idx_valid = idx_rand[self.training_samples:]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(features, edges_adj)\n",
    "            loss = self.loss_func(output[idx_train], labels[idx_train])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.model(features, edges_adj)\n",
    "                loss = self.loss_func(output[idx_valid], labels[idx_valid])\n",
    "\n",
    "\n",
    "# Initialize the configuration\n",
    "config = CoraConfig()\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Define your GAT model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_features, n_hidden, n_classes, n_heads, dropout):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat = GAT(in_features, n_hidden, n_classes, n_heads, dropout)\n",
    "\n",
    "    def forward(self, x, adj_mat):\n",
    "        return self.gat(x, adj_mat)\n",
    "\n",
    "# Create your GAT model and optimizer\n",
    "config.in_features = config.dataset.features.shape[1]\n",
    "config.n_classes = len(config.dataset.classes)\n",
    "config.model = GATModel(config.in_features, config.n_hidden, config.n_classes, config.n_heads, config.dropout)\n",
    "config.optimizer = Adam(config.model.parameters(), lr=5e-3, weight_decay=5e-4)\n",
    "\n",
    "PATH=\"weights\"\n",
    "config.model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3754631168 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lenes\\Desktop\\HW M2\\GAT\\eval_model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m idx_valid \u001b[39m=\u001b[39m idx_rand[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]  \u001b[39m# Use the remaining samples as the validation set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Perform model evaluation on the validation set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m validation_loss, validation_accuracy \u001b[39m=\u001b[39m evaluate(config\u001b[39m.\u001b[39mmodel, features, labels, idx_valid)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss: \u001b[39m\u001b[39m{\u001b[39;00mvalidation_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mvalidation_accuracy \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\lenes\\Desktop\\HW M2\\GAT\\eval_model.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m model(data, config\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39madj_mat\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mloss_func(output[idx_valid], labels[idx_valid])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     acc \u001b[39m=\u001b[39m accuracy(output[idx_valid], labels[idx_valid])\n",
      "File \u001b[1;32mc:\\Users\\lenes\\miniconda3\\envs\\envML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenes\\miniconda3\\envs\\envML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lenes\\Desktop\\HW M2\\GAT\\eval_model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj_mat):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgat(x, adj_mat)\n",
      "File \u001b[1;32mc:\\Users\\lenes\\miniconda3\\envs\\envML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenes\\miniconda3\\envs\\envML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lenes\\Desktop\\HW M2\\GAT\\eval_model.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj_mat):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x, adj_mat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\lenes\\miniconda3\\envs\\envML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenes\\miniconda3\\envs\\envML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lenes\\Desktop\\HW M2\\GAT\\eval_model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m g_repeat_interleave \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mrepeat_interleave(n_nodes, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# Now we concatenate to get\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# $$\\{\\overrightarrow{g_1} \\Vert \\overrightarrow{g_1},\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# \\overrightarrow{g_1} \\Vert \\overrightarrow{g_2},\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m# \\overrightarrow{g_2} \\Vert \\overrightarrow{g_2},\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# \\dots, \\overrightarrow{g_2}  \\Vert \\overrightarrow{g_N}, ...\\}$$\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m g_concat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([g_repeat_interleave, g_repeat], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lenes/Desktop/HW%20M2/GAT/eval_model.ipynb#W2sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m g_concat \u001b[39m=\u001b[39m g_concat\u001b[39m.\u001b[39mview(n_nodes, n_nodes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_hidden)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3754631168 bytes."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate(model, data, idx_valid):\n",
    "    features = data.features.to(config.device)\n",
    "    labels = data.labels.to(config.device)\n",
    "    edges_adj = data.adj_mat.to(config.device)\n",
    "    edges_adj = edges_adj.unsqueeze(-1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = config.model(features, edges_adj)\n",
    "        loss = config.loss_func(output[idx_valid], labels[idx_valid])\n",
    "        acc = accuracy(output[idx_valid], labels[idx_valid])\n",
    "    return loss.item(), acc\n",
    "\n",
    "# Define your evaluation set (validation set)\n",
    "labels = config.dataset.labels.to(config.device)\n",
    "features = config.dataset.features.to(config.device)\n",
    "\n",
    "# Use the remaining samples as the validation set\n",
    "idx_valid = range(config.training_samples, len(labels))\n",
    "\n",
    "# Perform model evaluation on the validation set\n",
    "validation_loss, validation_accuracy = evaluate(config.model, config.dataset, idx_valid)\n",
    "\n",
    "print(f'Validation Loss: {validation_loss:.4f}')\n",
    "print(f'Validation Accuracy: {validation_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(edges_adj[idx])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#model.eval()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#    output = config.model(features, edges_adj)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#    loss = config.loss_func(output[idx_valid], labels[idx_valid])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#    acc = accuracy(output[idx_valid], labels[idx_valid])\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m evaluate_individual(config\u001b[38;5;241m.\u001b[39mmodel, config\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_individual(model, data, idx):\n",
    "    features = data.features.to(config.device)\n",
    "    labels = data.labels.to(config.device)\n",
    "    edges_adj = data.adj_mat.to(config.device)\n",
    "    edges_adj = edges_adj.unsqueeze(-1)\n",
    "    print(labels[idx])\n",
    "    print(features[idx])\n",
    "    print(edges_adj[idx])\n",
    "    #model.eval()\n",
    "    #with torch.no_grad():\n",
    "    #    output = config.model(features, edges_adj)\n",
    "    #    loss = config.loss_func(output[idx_valid], labels[idx_valid])\n",
    "    #    acc = accuracy(output[idx_valid], labels[idx_valid])\n",
    "  \n",
    "evaluate_individual(config.model, config.dataset, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
