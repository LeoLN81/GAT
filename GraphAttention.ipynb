{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    ## Graph attention layer\n",
    "\n",
    "    This is a single graph attention layer.\n",
    "    A GAT is made up of multiple such layers.\n",
    "\n",
    "    It takes\n",
    "    $$\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$$,\n",
    "    where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input\n",
    "    and outputs\n",
    "    $$\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$$,\n",
    "    where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `n_heads`, $K$, is the number of attention heads\n",
    "        * `is_concat` whether the multi-head results should be concatenated or averaged\n",
    "        * `dropout` is the dropout probability\n",
    "        * `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Calculate the number of dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # If we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # If we are averaging the multiple heads\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # Linear layer for initial transformation;\n",
    "        # i.e. to transform the node embeddings before self-attention\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        # Linear layer to compute attention score $e_{ij}$\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        # The activation for attention score $e_{ij}$\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        # Softmax to compute attention $\\alpha_{ij}$\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n",
    "        * `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n",
    "        We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n",
    "\n",
    "        Adjacency matrix represent the edges (or connections) among nodes.\n",
    "        `adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        # The initial transformation,\n",
    "        # $$\\overrightarrow{g^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$$\n",
    "        # for each head.\n",
    "        # We do single linear transformation and then split it up for each head.\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        # #### Calculate attention score\n",
    "        #\n",
    "        # We calculate these for each head $k$. *We have omitted $\\cdot^k$ for simplicity*.\n",
    "        #\n",
    "        # $$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =\n",
    "        # a(\\overrightarrow{g_i}, \\overrightarrow{g_j})$$\n",
    "        #\n",
    "        # $e_{ij}$ is the attention score (importance) from node $j$ to node $i$.\n",
    "        # We calculate this for each head.\n",
    "        #\n",
    "        # $a$ is the attention mechanism, that calculates the attention score.\n",
    "        # The paper concatenates\n",
    "        # $\\overrightarrow{g_i}$, $\\overrightarrow{g_j}$\n",
    "        # and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$\n",
    "        # followed by a $\\text{LeakyReLU}$.\n",
    "        #\n",
    "        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "        # \\mathbf{a}^\\top \\Big[\n",
    "        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n",
    "        # \\Big] \\Big)$$\n",
    "\n",
    "        # First we calculate\n",
    "        # $\\Big[\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j} \\Big]$\n",
    "        # for all pairs of $i, j$.\n",
    "        #\n",
    "        # `g_repeat` gets\n",
    "        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N},\n",
    "        # \\overrightarrow{g_1}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_N}, ...\\}$$\n",
    "        # where each node embedding is repeated `n_nodes` times.\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "        # `g_repeat_interleave` gets\n",
    "        # $$\\{\\overrightarrow{g_1}, \\overrightarrow{g_1}, \\dots, \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_2}, \\overrightarrow{g_2}, \\dots, \\overrightarrow{g_2}, ...\\}$$\n",
    "        # where each node embedding is repeated `n_nodes` times.\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        # Now we concatenate to get\n",
    "        # $$\\{\\overrightarrow{g_1} \\Vert \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_1} \\Vert \\overrightarrow{g_2},\n",
    "        # \\dots, \\overrightarrow{g_1}  \\Vert \\overrightarrow{g_N},\n",
    "        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_1},\n",
    "        # \\overrightarrow{g_2} \\Vert \\overrightarrow{g_2},\n",
    "        # \\dots, \\overrightarrow{g_2}  \\Vert \\overrightarrow{g_N}, ...\\}$$\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        # Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "\n",
    "        # Calculate\n",
    "        # $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "        # \\mathbf{a}^\\top \\Big[\n",
    "        # \\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}\n",
    "        # \\Big] \\Big)$$\n",
    "        # `e` is of shape `[n_nodes, n_nodes, n_heads, 1]`\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        # Remove the last dimension of size `1`\n",
    "        e = e.squeeze(-1)\n",
    "\n",
    "        # The adjacency matrix should have shape\n",
    "        # `[n_nodes, n_nodes, n_heads]` or`[n_nodes, n_nodes, 1]`\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "        # Mask $e_{ij}$ based on adjacency matrix.\n",
    "        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
    "\n",
    "        # We then normalize attention scores (or coefficients)\n",
    "        # $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =\n",
    "        # \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
    "        #\n",
    "        # where $\\mathcal{N}_i$ is the set of nodes connected to $i$.\n",
    "        #\n",
    "        # We do this by setting unconnected $e_{ij}$ to $- \\infty$ which\n",
    "        # makes $\\exp(e_{ij}) \\sim 0$ for unconnected pairs.\n",
    "        a = self.softmax(e)\n",
    "\n",
    "        # Apply dropout regularization\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        # Calculate final output for each head\n",
    "        # $$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{g^k_j}$$\n",
    "        #\n",
    "        # *Note:* The paper includes the final activation $\\sigma$ in $\\overrightarrow{h_i}$\n",
    "        # We have omitted this from the Graph Attention Layer implementation\n",
    "        # and use it on the GAT model to match with how other PyTorch modules are defined -\n",
    "        # activation as a separate layer.\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            # $$\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        # Take the mean of the heads\n",
    "        else:\n",
    "            # $$\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$$\n",
    "            return attn_res.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from download import download_file, extract_tar\n",
    "\n",
    "class CoraDataset:\n",
    "    def __init__(self, include_edges: bool = True):\n",
    "        self.include_edges = include_edges\n",
    "\n",
    "        # Download dataset\n",
    "        self._download()\n",
    "\n",
    "        # Read the paper ids, feature vectors, and labels\n",
    "        content = np.genfromtxt('cora/cora/cora.content', dtype=np.dtype(str))\n",
    "\n",
    "        # Load the citations, it's a list of pairs of integers.\n",
    "        citations = np.genfromtxt('cora/cora/cora.cites', dtype=np.int32)\n",
    "\n",
    "        features = torch.tensor(np.array(content[:, 1:-1], dtype=np.float32))\n",
    "        self.features = features / features.sum(dim=1, keepdim=True)\n",
    "\n",
    "        self.classes = {s: i for i, s in enumerate(set(content[:, -1]))}\n",
    "        self.labels = torch.tensor([self.classes[i] for i in content[:, -1]], dtype=torch.long)\n",
    "\n",
    "        paper_ids = np.array(content[:, 0], dtype=np.int32)\n",
    "        ids_to_idx = {id_: i for i, id_ in enumerate(paper_ids)}\n",
    "\n",
    "        self.adj_mat = torch.eye(len(self.labels), dtype=torch.bool)\n",
    "\n",
    "        if self.include_edges:\n",
    "            for e in citations:\n",
    "                e1, e2 = ids_to_idx[e[0]], ids_to_idx[e[1]]\n",
    "                self.adj_mat[e1][e2] = True\n",
    "                self.adj_mat[e2][e1] = True\n",
    "\n",
    "    def _download(self):\n",
    "        data_dir = Path('cora')\n",
    "        if not data_dir.exists():\n",
    "            data_dir.mkdir(parents=True)\n",
    "\n",
    "        tgz_file = data_dir / 'cora.tgz'\n",
    "        content_file = data_dir / 'cora.content'\n",
    "\n",
    "        if not content_file.exists():\n",
    "            download_file('https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz', tgz_file)\n",
    "            extract_tar(tgz_file, data_dir)\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = GraphAttentionLayer(in_features, n_hidden, n_heads, is_concat=True, dropout=dropout)\n",
    "        self.activation = nn.ELU()\n",
    "        self.output = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, adj_mat):\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer1(x, adj_mat)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x, adj_mat)\n",
    "\n",
    "\n",
    "def accuracy(output: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    A simple function to calculate the accuracy\n",
    "    \"\"\"\n",
    "    return output.argmax(dim=-1).eq(labels).sum().item() / len(labels)\n",
    "\n",
    "\n",
    "class CoraConfig:\n",
    "    def __init__(self):\n",
    "        self.include_edges = True\n",
    "        self.epochs = 10\n",
    "        self.model = None\n",
    "        self.training_samples = 500\n",
    "        self.in_features = None\n",
    "        self.n_hidden = 64\n",
    "        self.n_heads = 8\n",
    "        self.n_classes = None\n",
    "        self.dropout = 0.6\n",
    "        self.dataset = CoraDataset(self.include_edges)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.optimizer = None #torch.optim.Adam(self.model.parameters())\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "\n",
    "    def run(self):\n",
    "        features = self.dataset.features.to(self.device)\n",
    "        labels = self.dataset.labels.to(self.device)\n",
    "        edges_adj = self.dataset.adj_mat.to(self.device)\n",
    "        edges_adj = edges_adj.unsqueeze(-1)\n",
    "\n",
    "        idx_rand = torch.randperm(len(labels))\n",
    "        idx_train = idx_rand[:self.training_samples]\n",
    "        idx_valid = idx_rand[self.training_samples:]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(features, edges_adj)\n",
    "            loss = self.loss_func(output[idx_train], labels[idx_train])\n",
    "            print(\"loss train:\", loss.item())\n",
    "            self.losses_train.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.model(features, edges_adj)\n",
    "                loss = self.loss_func(output[idx_valid], labels[idx_valid])\n",
    "                print(\"loss valid:\", loss.item())\n",
    "                self.losses_valid.append(loss.item())\n",
    "\n",
    "\n",
    "# Initialize the configuration\n",
    "config = CoraConfig()\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Define your GAT model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_features, n_hidden, n_classes, n_heads, dropout):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat = GAT(in_features, n_hidden, n_classes, n_heads, dropout)\n",
    "\n",
    "    def forward(self, x, adj_mat):\n",
    "        return self.gat(x, adj_mat)\n",
    "\n",
    "# Create your GAT model and optimizer\n",
    "config.in_features = config.dataset.features.shape[1]\n",
    "config.n_classes = len(config.dataset.classes)\n",
    "config.model = GATModel(config.in_features, config.n_hidden, config.n_classes, config.n_heads, config.dropout)\n",
    "config.optimizer = Adam(config.model.parameters(), lr=7e-3, weight_decay=5e-4)\n",
    "\n",
    "config.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
